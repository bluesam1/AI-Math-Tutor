# Story 2.4: LLM-Based Answer Detection Guardrail

## Status

Pending

## Story

**As a** student,
**I want** the system to detect implicit answers that keyword matching might miss,
**so that** the Socratic principles are maintained even when answers are phrased subtly.

## Acceptance Criteria

1: A backend service module is created for LLM-based answer validation (e.g., `answerValidationService.ts`) in TypeScript that uses a secondary LLM call to analyze response context.

2: The validation service sends the LLM-generated response to a secondary LLM call with a prompt asking:

- "Does this response contain a direct answer to the math problem?"
- "Would this response give away the solution without requiring student thinking?"

3: The secondary LLM call analyzes the response context and returns a validation result (contains answer / does not contain answer).

4: When the validation detects an implicit answer, the service flags the response for blocking or rewriting.

5: The validation is applied to all LLM-generated responses after keyword-based detection, providing a second layer of protection.

6: The validation service includes error handling for secondary LLM API failures, defaulting to blocking uncertain responses when validation cannot be performed.

7: The service returns a validation result indicating whether the response contains implicit answers, along with confidence level if available.

8: The validation is optimized for response time, using efficient prompts and parameters to minimize additional latency.

## Tasks / Subtasks

- [ ] Task 1: Create Answer Validation Service Module (AC: 1)
  - [ ] Create `functions/src/services/answerValidationService.ts`
  - [ ] Define TypeScript interfaces for validation results
  - [ ] Create service structure with validation functions
  - [ ] Export validation functions

- [ ] Task 2: Design Validation Prompt (AC: 2)
  - [ ] Create prompt template for answer detection validation
  - [ ] Include question: "Does this response contain a direct answer to the math problem?"
  - [ ] Include question: "Would this response give away the solution without requiring student thinking?"
  - [ ] Include the LLM-generated response to analyze
  - [ ] Include the original problem statement for context
  - [ ] Optimize prompt for clarity and efficiency

- [ ] Task 3: Implement Validation LLM Call (AC: 1, 3)
  - [ ] Use LLM service from Story 2.1
  - [ ] Create `validateResponse()` function
  - [ ] Accept LLM-generated response and problem statement as input
  - [ ] Build validation prompt
  - [ ] Call LLM API with validation prompt
  - [ ] Parse LLM response to extract validation result

- [ ] Task 4: Parse Validation Result (AC: 3, 7)
  - [ ] Parse LLM response to determine if answer is present
  - [ ] Extract confidence level if available
  - [ ] Handle structured responses (JSON) or natural language responses
  - [ ] Create validation result interface:
    - `containsAnswer: boolean`
    - `confidence: number` (if available)
    - `reasoning: string` (optional, for debugging)

- [ ] Task 5: Flag Responses with Answers (AC: 4)
  - [ ] When validation detects answer, flag response for blocking
  - [ ] Return validation result with flag
  - [ ] Include detected answer information in result
  - [ ] Handle edge cases (uncertain responses)

- [ ] Task 6: Integrate with Keyword Detection (AC: 5)
  - [ ] Apply validation after keyword-based detection (Story 2.3)
  - [ ] Only run validation if keyword detection doesn't flag response (optimization)
  - [ ] Or run validation on all responses for thoroughness
  - [ ] Combine detection results from both methods

- [ ] Task 7: Implement Error Handling (AC: 6)
  - [ ] Handle LLM API failures (network errors, API errors, timeouts)
  - [ ] Default to blocking uncertain responses when validation fails
  - [ ] Log validation failures for monitoring
  - [ ] Return safe default result when validation cannot be performed
  - [ ] Handle timeout errors gracefully

- [ ] Task 8: Optimize Response Time (AC: 8)
  - [ ] Use efficient prompt (concise, clear)
  - [ ] Configure LLM parameters for faster responses (lower max_tokens, etc.)
  - [ ] Consider using faster/cheaper model for validation (if available)
  - [ ] Cache validation prompts
  - [ ] Monitor and optimize validation latency

- [ ] Task 9: Add Confidence Scoring (AC: 7)
  - [ ] Extract confidence level from LLM response if available
  - [ ] Use confidence to determine blocking threshold
  - [ ] Return confidence in validation result
  - [ ] Use confidence for logging and monitoring

- [ ] Task 10: Add Unit Tests (AC: 1-8)
  - [ ] Create test file `functions/src/services/__tests__/answerValidationService.test.ts`
  - [ ] Test validation prompt generation
  - [ ] Test validation result parsing
  - [ ] Test error handling scenarios
  - [ ] Mock LLM service responses
  - [ ] Test with various response types

- [ ] Task 11: Integration Testing (AC: 1-8)
  - [ ] Test validation with real LLM responses
  - [ ] Test with responses that contain implicit answers
  - [ ] Test with responses that don't contain answers
  - [ ] Test error handling with API failures
  - [ ] Verify response time is acceptable
  - [ ] Test across different problem types

- [ ] Task 12: Performance Testing (AC: 8)
  - [ ] Measure validation latency
  - [ ] Test with different LLM models
  - [ ] Optimize prompt and parameters for speed
  - [ ] Ensure validation doesn't significantly slow down response time
  - [ ] Document performance characteristics

## Dependencies

- Story 2.1: LLM Integration Backend Service (uses LLM service for validation)

## Required By

- Story 2.5: Answer Blocking & Response Rewriting (will use this validation service)

## Notes

- This is a second-tier guardrail (context-aware, LLM-based)
- Works in combination with keyword-based detection (Story 2.3)
- Should default to blocking when uncertain (better safe than sorry)
- Response time optimization is critical - don't want to slow down dialogue significantly
- Consider cost implications of additional LLM call per response


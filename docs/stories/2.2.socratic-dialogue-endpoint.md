# Story 2.2: Socratic Dialogue Generation Endpoint

## Status

Pending

## Story

**As a** student,
**I want** to receive Socratic guiding questions when I submit my problem,
**so that** I can start working through the solution with guided questions.

## Acceptance Criteria

1: A backend API endpoint (e.g., POST /api/chat/message) accepts student messages and generates Socratic dialogue responses using the LLM service.

2: The endpoint accepts the current problem statement and student message as input, along with conversation context (last 10 messages if available).

3: The LLM prompt includes:

- The current problem statement
- The problem type (arithmetic, algebra, geometry, word problems, multi-step)
- Conversation history (last 10 messages)
- Instructions to generate Socratic guiding questions, never direct answers
- Instructions to use progressive disclosure and chain-of-thought strategies

4: The endpoint returns the generated Socratic dialogue response to the frontend.

5: Error handling is implemented for LLM API failures, returning appropriate error messages to the frontend.

6: The endpoint handles empty or invalid student messages gracefully, prompting for clarification.

7: The response includes metadata indicating whether the response is a question, hint, or encouragement (for visual feedback purposes).

8: The endpoint validates that the problem statement exists before generating dialogue (returns error if no problem is set).

## Tasks / Subtasks

- [ ] Task 1: Create Chat Route (AC: 1)
  - [ ] Create `functions/src/routes/chat.ts` (or update existing chat route)
  - [ ] Define POST `/api/chat/message` endpoint
  - [ ] Set up route handler structure
  - [ ] Add route to Express app

- [ ] Task 2: Create Chat Controller (AC: 1)
  - [ ] Create `functions/src/controllers/chatController.ts`
  - [ ] Implement `handleMessage()` function
  - [ ] Extract request body (problem statement, student message, session ID)
  - [ ] Validate request body structure
  - [ ] Return appropriate responses

- [ ] Task 3: Implement Request Validation (AC: 2, 6, 8)
  - [ ] Validate problem statement exists (return error if not set)
  - [ ] Validate student message is not empty
  - [ ] Validate student message is not just whitespace
  - [ ] Return appropriate error messages for invalid input
  - [ ] Handle missing or malformed request body

- [ ] Task 4: Integrate Context Retrieval (AC: 2)
  - [ ] Accept session ID from request
  - [ ] Retrieve conversation context (last 10 messages) for session
  - [ ] Handle empty context (new conversation)
  - [ ] Format conversation history for LLM prompt
  - [ ] Note: Context service will be implemented in Story 2.6, use placeholder for now

- [ ] Task 5: Build LLM Prompt (AC: 3)
  - [ ] Include current problem statement in prompt
  - [ ] Include problem type in prompt
  - [ ] Include conversation history (last 10 messages) in prompt
  - [ ] Include instructions for Socratic guiding questions
  - [ ] Include instructions to never give direct answers
  - [ ] Include instructions for progressive disclosure
  - [ ] Include instructions for chain-of-thought strategies

- [ ] Task 6: Call LLM Service (AC: 1, 4)
  - [ ] Import `llmService` from Story 2.1
  - [ ] Call `generateSocraticDialogue()` with appropriate parameters
  - [ ] Handle LLM service response
  - [ ] Extract generated dialogue text

- [ ] Task 7: Add Response Metadata (AC: 7)
  - [ ] Analyze response to determine type (question, hint, encouragement)
  - [ ] Add metadata to response object
  - [ ] Return metadata along with dialogue text
  - [ ] Format response for frontend consumption

- [ ] Task 8: Implement Error Handling (AC: 5)
  - [ ] Handle LLM API failures (network errors, API errors, timeouts)
  - [ ] Return appropriate error messages to frontend
  - [ ] Use consistent error response format
  - [ ] Log errors for debugging
  - [ ] Handle edge cases gracefully

- [ ] Task 9: Handle Empty/Invalid Messages (AC: 6)
  - [ ] Check if student message is empty or only whitespace
  - [ ] Return clarification prompt (e.g., "Could you tell me more about what you're thinking?")
  - [ ] Handle messages that are too short or unclear
  - [ ] Provide helpful guidance for unclear messages

- [ ] Task 10: Integrate with Problem Validation (AC: 8)
  - [ ] Check if problem statement exists in request
  - [ ] Return error if no problem is set
  - [ ] Verify problem statement is valid format
  - [ ] Include problem metadata (type, etc.) in prompt

- [ ] Task 11: Add Request/Response Types (AC: 1-8)
  - [ ] Define TypeScript interfaces for request body
  - [ ] Define TypeScript interfaces for response
  - [ ] Add type definitions to `functions/src/types/api.ts`
  - [ ] Ensure type safety throughout endpoint

- [ ] Task 12: Add Unit Tests (AC: 1-8)
  - [ ] Create test file `functions/src/controllers/__tests__/chatController.test.ts`
  - [ ] Test request validation
  - [ ] Test LLM service integration
  - [ ] Test error handling
  - [ ] Test response formatting
  - [ ] Mock LLM service responses

- [ ] Task 13: Integration Testing (AC: 1-8)
  - [ ] Test endpoint with valid requests
  - [ ] Test endpoint with invalid requests
  - [ ] Test LLM integration with real API calls
  - [ ] Test error scenarios
  - [ ] Verify response format and metadata

## Dependencies

- Story 2.1: LLM Integration Backend Service

## Required By

- Story 2.6: Context Management Service (will integrate context retrieval)
- Story 2.7: Chat UI Component (consumes this endpoint)
- Story 2.8: Progressive Help Escalation Logic (will enhance prompts)

## Notes

- Context service (Story 2.6) will be integrated later - use placeholder for now
- Response metadata will be used by Chat UI (Story 2.7) for visual feedback
- Error messages should be age-appropriate for 6th grade students
- Consider rate limiting per session to prevent abuse

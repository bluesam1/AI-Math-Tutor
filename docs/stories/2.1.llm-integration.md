# Story 2.1: LLM Integration Backend Service

## Status

Pending

## Story

**As a** developer,
**I want** to integrate an LLM API (OpenAI GPT-4, Claude, or similar) for generating Socratic dialogue,
**so that** the system can generate guiding questions that help students discover solutions.

## Acceptance Criteria

1: A backend service module is created for LLM API integration (e.g., `llmService.ts`) in TypeScript that handles API calls to the LLM service (OpenAI GPT-4, Claude, or similar).

2: The LLM service is configured with appropriate API keys stored securely in environment variables, never exposed in client-side code.

3: The service includes a function for generating Socratic dialogue responses with appropriate prompts that instruct the LLM to:

- Generate guiding questions, not direct answers
- Use progressive disclosure and chain-of-thought strategies
- Break down problems appropriately
- Adapt to student understanding level

4: The service handles API rate limits gracefully, implementing retry logic or queuing when rate limits are reached.

5: Error handling is implemented for LLM API failures (network errors, API errors, timeouts) with appropriate error messages.

6: The service includes response time monitoring to ensure responses are generated within 3 seconds when possible.

7: The service is structured to support different LLM providers (OpenAI, Claude, etc.) with provider-specific configuration.

8: API requests include appropriate parameters (temperature, max tokens, etc.) optimized for Socratic dialogue generation.

## Tasks / Subtasks

- [ ] Task 1: Create LLM Service Module Structure (AC: 1, 7)
  - [ ] Create `functions/src/services/llmService.ts`
  - [ ] Define TypeScript interfaces for LLM providers (OpenAI, Claude, etc.)
  - [ ] Create provider abstraction layer for multiple LLM providers
  - [ ] Define response types for LLM responses
  - [ ] Set up provider-specific configuration structure

- [ ] Task 2: Configure Environment Variables (AC: 2)
  - [ ] Add LLM API key to environment variables (e.g., `OPENAI_API_KEY` or `CLAUDE_API_KEY`)
  - [ ] Update `functions/src/config/env.ts` to include LLM configuration
  - [ ] Verify API keys are never exposed in client-side code
  - [ ] Add environment variable validation
  - [ ] Document required environment variables in README

- [ ] Task 3: Implement OpenAI Provider (AC: 1, 7)
  - [ ] Install OpenAI SDK (if using OpenAI)
  - [ ] Create OpenAI provider implementation
  - [ ] Implement API call function with proper authentication
  - [ ] Configure OpenAI-specific parameters (model, temperature, max_tokens)
  - [ ] Test OpenAI API connection

- [ ] Task 4: Implement Claude Provider (AC: 1, 7)
  - [ ] Install Anthropic SDK (if using Claude)
  - [ ] Create Claude provider implementation
  - [ ] Implement API call function with proper authentication
  - [ ] Configure Claude-specific parameters
  - [ ] Test Claude API connection

- [ ] Task 5: Create Socratic Dialogue Prompt Template (AC: 3)
  - [ ] Design prompt template for Socratic dialogue generation
  - [ ] Include instructions to generate guiding questions, not direct answers
  - [ ] Include instructions for progressive disclosure and chain-of-thought strategies
  - [ ] Include instructions to break down problems appropriately
  - [ ] Include instructions to adapt to student understanding level
  - [ ] Create prompt builder function

- [ ] Task 6: Implement Socratic Dialogue Generation Function (AC: 3)
  - [ ] Create `generateSocraticDialogue()` function
  - [ ] Accept problem statement, problem type, and conversation context as parameters
  - [ ] Build prompt using Socratic dialogue template
  - [ ] Call LLM API with proper parameters
  - [ ] Return generated dialogue response
  - [ ] Handle different problem types appropriately

- [ ] Task 7: Implement Rate Limiting and Retry Logic (AC: 4)
  - [ ] Add rate limit detection logic
  - [ ] Implement exponential backoff retry strategy
  - [ ] Add request queuing for rate limit scenarios
  - [ ] Set maximum retry attempts
  - [ ] Log rate limit events for monitoring

- [ ] Task 8: Implement Error Handling (AC: 5)
  - [ ] Handle network errors (connection failures, timeouts)
  - [ ] Handle API errors (invalid API key, quota exceeded, etc.)
  - [ ] Handle timeout errors (requests taking too long)
  - [ ] Create custom error types for different error scenarios
  - [ ] Return appropriate error messages
  - [ ] Log errors for debugging

- [ ] Task 9: Implement Response Time Monitoring (AC: 6)
  - [ ] Add timing logic to track response generation time
  - [ ] Log response times for monitoring
  - [ ] Set warning threshold for slow responses (>3 seconds)
  - [ ] Optimize prompt and parameters for faster responses
  - [ ] Add metrics collection for response times

- [ ] Task 10: Configure LLM Parameters (AC: 8)
  - [ ] Set temperature parameter (optimized for Socratic dialogue)
  - [ ] Set max_tokens parameter (to control response length)
  - [ ] Configure other provider-specific parameters
  - [ ] Test different parameter combinations
  - [ ] Document parameter choices and rationale

- [ ] Task 11: Add Unit Tests (AC: 1-8)
  - [ ] Create test file `functions/src/services/__tests__/llmService.test.ts`
  - [ ] Test prompt generation
  - [ ] Test error handling scenarios
  - [ ] Test rate limiting and retry logic
  - [ ] Mock LLM API responses
  - [ ] Test provider abstraction layer

- [ ] Task 12: Integration Testing (AC: 1-8)
  - [ ] Test actual LLM API calls (with test API key)
  - [ ] Verify Socratic dialogue generation quality
  - [ ] Test error handling with real API errors
  - [ ] Test rate limiting with actual rate limits
  - [ ] Verify response times are acceptable

## Dependencies

- None (foundational story)

## Required By

- Story 2.2: Socratic Dialogue Generation Endpoint
- Story 2.4: LLM-Based Answer Detection Guardrail
- Story 2.5: Answer Blocking & Response Rewriting

## Notes

- Need to decide between OpenAI GPT-4 or Claude for initial implementation
- Consider cost implications of different providers
- Response time target: <3 seconds when possible
- Provider abstraction allows switching providers later if needed
